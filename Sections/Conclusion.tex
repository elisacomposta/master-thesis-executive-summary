\section{Conclusions}
\label{sec:conclusion}

% Intro
This work explored the use of LLMs as agents in social simulations, by extending the original simulator to integrate opinion modeling, misinformation agents, and a realistic initialization based on real-world data.

% Main results
Results showed that LLM-based agents are capable of interacting, generating content with different linguistic styles, and forming social connections. 
Opinion scores assigned by LLMs followed the trends of traditional opinion dynamics models, supporting their validity for population-level analysis.

% Limitations
However, some limitations emerged.
The simulated time was not sufficient to observe long-term effects, and the network was not sufficiently structured for the recommendation systems to have an impact. 
Moreover, agents lacked the cognitive and emotional mechanisms necessary to reproduce the susceptibility to misinformation, which was negligible in the simulations.

% Future work
Future research could explore a wider range of disinformation strategies (e.g., bots, coordinated groups), integrate multimodal content (text, images, videos), or introduce external events during the simulation timeline. 
Additionally, a comparison with real-world data would help assess the realism of the emergent behaviors and validate LLM-based agents.

% Conclusion
Overall, this work shows that LLMs are a powerful tool for simulating social phenomena, enabling more realistic modeling of interactions in agent-based systems.
However, replicating complex dynamics remains a challenge, and further studies are required to make these simulations closer to real-world dynamics.
